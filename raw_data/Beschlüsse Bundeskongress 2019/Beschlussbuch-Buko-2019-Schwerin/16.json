{"title": "GOOGLE UND DIE FRAU AM HERD – AUCH DAS DIGITALE MUSS DISKRIMINIERUNGSFREI SEIN!", "text_html": "<p>Dass nicht nur Menschen diskriminieren können, sondern auch Suchmaschinen, zeigt sich, wenn man bei Google „Frauen* sollten“ oder „Frauen* müssen“ eingibt und die automatische Vervollständigung abwartet. Da sollen Frauen* „sich rar machen“, „sich ihren Männern* unterordnen“, „zuhause bleiben“ oder „Kurven haben“. Aber auch wenn man die automatische Vervollständigung weglässt, ploppt als erstes Suchergebnis eine 10-Punkte-Liste eines bekannten Datingportals auf, die eine Frau* abarbeiten sollte, wenn sie „Männer* um den kleinen Finger wickeln“ will. Wenn wir „CEO“ in die Google-Bildersuche eingeben, erscheinen überproportional viele Bilder von weißen Männern*. In einer Untersuchung hat die University of Washington ausgerechnet, dass in der Bildersuche lediglich 11 Prozent Frauen* unter dem Suchbegriff „CEO“ auftauchen. In der Realität sind etwa 27 Prozent aller CEOs in den USA weiblich. Diskriminierung durch Onlinedienste treffen aber nicht nur Frauen*. So wurden Fotos von schwarzen Personen von Flickr mit dem Schlagwort „Affe“ und von Google mit dem Schlagwort „Gorilla“ versehen. Wie kann sowas passieren? Wer uns auf Facebook als Freund*in vorgeschlagen oder welche Einkaufsempfehlung uns bei Amazon angezeigt wird, wird auf Basis von Algorithmen entschieden. Algorithmen sind eine Folge von Anweisungen und Rechenoperationen, die eine bestimmte Vorgehensweise durchführen, um ein Problem zu lösen. Sie sollen systematisch sein, logisch und auch beim tausendsten Mal noch zum selben Ergebnis führen. Sie sind eindeutige Handlungsvorschriften für die Lösung eines Problems oder einer Klasse von Problemen. Ziel ist die Reduktion von Komplexität und ein maßgeschneidertes Output, so dass durch einen algorithmenbasierten Suchprozess das gefunden wird, was wir auch wirklich suchen. Wenn wir in die Suchmaschine die Frage nach Parkplätzen in der Umgebung des Standortes eingeben, werden uns Informationen zu möglichen Parkhäusern bereitgestellt. Bei der Beantwortung dieser Frage werden uns auch bei der zehnten oder hundertsten Frage die gleichen Parkhäuser angezeigt, bis vielleicht irgendwann ein weiteres in der Umgebung öffnet. Und genau die gleichen Parkhäuser werden auch jedem*jeder anderen Nutzer*in angezeigt. Diese Algorithmen sind erst einmal neutral und müssen mit Datensätzen gefüttert werden, um ein Output zu liefern. Algorithmische Systeme lernen dadurch, dass immer wieder und wieder dasselbe Verfahren nach festgelegten Kriterien durchgeführt wird. Das Füttern von Algorithmen kann entweder gezielt durch die Eingabe von Datensätzen erfolgen oder indem Individuen durch ihre Suchanfragen eine bestimmte Häufigkeit aufweisen. Wenn also viele Sucher*innen nach „Frauen* müssen Kurven haben“ suchen, erscheint diese Vervollständigung weiter oben als „Frauen* müssen zusammenhalten“. Algorithmen können für alles und jedes angewendet werden und werden dementsprechend geschrieben und programmiert. So finden Algorithmen Anwendung in der Medizin, indem sie Zellen untersuchen und krebserkrankte Zellen von gesunden Zellen unterscheiden können, oder in der technischen Aufrüstung von Fahrzeugen und deren Sicherheitssystemen, wie automatischen Bremshilfen oder Abstandshaltern. Problematisch werden Algorithmen dann, wenn sie so angewendet werden, dass es ethisch schwierig wird, zum Beispiel im Bereich Predictive Policing, wenn also Straftaten vorhergesagt werden sollen, der Vergabe von Kreditanträgen, dem Aussortieren von Bewerber*innen in Bewerbungsverfahren oder auch bei der Anwendung in der Rechtsprechung. Algorithmen prägen damit unsere Umwelt. Wie wir etwas sehen, etwas verstehen, etwas empfinden oder eine Meinung entwickeln, kann durch die Suchergebnisse auf unsere Fragen stetig beeinflusst werden. Suchergebnisse sind genauso wenig frei von der Fortschreibung der Geschlechterrollen und Stereotypen, wie auch</p><p>Juso-Bundeskongress 22. – 24. November 2019</p><p>Beschluss N2</p><p>die analoge Welt nicht frei von Rollenbildern und Vorurteilen ist. Als Jungsozialist*innen haben wir den Anspruch, unsere Umwelt so zu gestalten und zu beeinflussen, dass wir mit kleinen Schritten der Gleichstellung von Männern* und Frauen* näherkommen und uns gegen jegliche Diskriminierung von Menschen zu stellen. Deshalb möchten wir eine Diskussion für den Umgang mit der stereotypgebundenen, diskriminierenden Ausgestaltung des Netzes aufgrund von Algorithmen anstoßen.</p><p>WAS GESCHIEHT DA EIGENTLICH BEI MEINEN SUCHANFRAGEN?</p><p>Algorithmen sind eindeutige Handlungsvorschriften für die Lösung eines Problems oder einer Klasse von Problemen. Bei herkömmlichen Algorithmen ist es selbst bei den komplexesten Befehlsabläufen mindestens für Spezialist*inneon möglich, die Handlungen des Programms nachzuvollziehen. Dies ändert sich aber mit der Automatisierung intelligenten Verhaltens. Durch maschinelles Lernen entwickeln sich Algorithmen selbstständig weiter, sodass Nachvolziehbarkeit nicht mehr zwingend gegeben sein muss. Aufgrund des Eingreifens von automatisierten Entscheidungen in unsere Gesellschaft und unseren Alltag ist es entscheidend, dass algorithmische Entscheidungen sowie alle Daten, die diese Entscheidungen beeinflussen, den Endnutzer*innen und anderen Interessensgruppen erläutert werden können. Die Kontrolle des Algorithmus sowie eine Debatte über die erlernten Verfahrensabläufe scheitert oftmals bereits an der Intransparenz der Tech-Unternehmen bezüglich ihres Codes. So fordern Prof. Dr. Katharina Zweig, Professorin für Graphentheorie und Analyse komplexer Netzwerke an der TU Kaiserslautern und Lorena Jaume-Palasi, Philosophin an der LMU München mit den Forschungsschwerpunkten Rechtsphilosophie und politische Philosophie im digitalen Zeitalter, über eine Transparenzoffensive nachzudenken. So könnten Entwickler*innen von Algorithmen angeregt werden, den „Baukasten“ und die Zusammensetzung ihrer Algorithmen offen zu legen. Sie argumentieren, dass dies vergleichbar sein könnte mit der Einnahme von Medikamenten auf ihrem Beipackzettel: ich muss vielleicht nicht im Detail kennen und verstehen, wie ein Medikament auf eine bestimmte Krankheit wirkt, allerdings habe ich einen Anspruch, zumindest die Bestandteile zu erfahren. Wenn die Codes eines Algorithmus‘ also zumindest in ihren Teilen einer breiteren Öffentlichkeit, wenigstens aber neutralen Expert*innen zugänglich gemacht werden, können auch Menschen, die in diesem Gebiet nicht spezialisiert sind, anfangen zu verstehen, warum mir dies oder jenes angezeigt wird, wenn ich Begrifflichkeiten in Suchmaschinen eingebe. Das kann natürlich nur funktionieren, wenn Menschen, die mit Informatik nichts am Hut haben, zum einen Sensibilität im Umgang mit der digitalen Welt, zum anderen Medienkompetenz erlangen können. Hierfür ist es unabdingbar, dass Menschen bereits von Kindesalter an den Umgang mit digitalen Medien lernen und sich im Laufe des Lebens in diesem Bereich fortbilden können. Wir fordern weiterhin Digitalisierung als Querschnittsthema in die Lehrpläne aufzunehmen, Schulen, Hochschulen und andere Bildungsorte mit der nötigen Infrastruktur für das Erlernen von Medienkompetenz auszustatten, Lehrkräften für diesen Bereich Fortbildungsmaßnahmen zur Verfügung zu stellen und Digitalisierung als eines der zentralen Themen der Weiterbildung zu begreifen und Angebote zu schaffen.</p><p>DISKRIMINIERUNG IN ALGORITHMISCHEN SYSTEMEN</p><p>Bei der Entwicklung lernender Systeme werden zusätzliche Annahmen getroffen, die ebenfalls eine verzerrende Wirkung entfalten können. So setzt der Lernprozess auf Daten aus der Vergangenheit auf, die nicht zwangsläufig heutige Zielvorstellung enthalten. Wenn z. B. in der Vergangenheit bevorzugt Männer gegenüber Frauen eingestellt wurden, so wird sich dieses Verhalten auch in daraus resultierende Entscheidung wiederfinden. Zusätzlich gilt es zu beachten, dass Daten immer mit zufälligen oder systematisch Fehlern in Ihrer Erhebung versehen sein können, was sich ebenfalls verzerrend auswirken kann.</p><p>FRAUEN* IN DIE INFORMATIK!</p><p>In der Forschung zu Gender und Digitalisierung wird immer wieder festgestellt, dass die MINTBereiche nach wie vor stark männlich dominiert sind und Frauen* leider allzu oft nur eine sehr kleine Rolle spielen. Dabei war insbesondere die Informatik zu Beginn überwiegend von Frauen* dominiert. Mit Ada Lovelace, die 1842 einen Algorithmus für eine Rechenmaschine entwarf, über Grace Hopper, die mit ihrer Arbeit an den ersten Großrechenanlagen den Begriff des „Debugging“, dem Diagnostizieren und Auffinden von Fehlern in Computersystemen, prägte und über die fast ausschließliche Durchführung der Tätigkeit des Programmierens durch Frauen* während des Zweiten Weltkriegs, stagniert der geringe Anteil von Frauen* in MINT-Ausbildungsberufen und Studiengängen weiterhin zwischen 15 und 20 Prozent. Laut einer Studie der Universität Bamberg arbeiten in zahlreichen IT-Abteilungen von Unternehmen weniger als ein Zehntel Frauen*. Frauen* in leitenden Positionen der großen Internetfirmen wie Marissa Mayer, ehemals CEO von Yahoo, stellen leider die Ausnahme von der Regel dar. Die Erhöhung des Frauen*anteils in MINT-Berufen, insbesondere in der Informatik-Branche ist für uns Jusos nach wie vor ein Anliegen. Wir unterstützen Maßnahmen,</p><p>Juso-Bundeskongress 22. – 24. November 2019</p><p>Beschluss N2</p><p>Programme und Netzwerke, die sich für die Förderung des Frauen*anteils einsetzen, wie das Projekt GEWINN (Gender/ Wissen/Informatik/Netzwerk) oder die Hochschulforschung zu Gender und IT an verschiedenen Hochschulen. Weiterhin muss ein gesellschaftlicher Wandel in Hinblick auf das Verständnis und die Stereotypen von MINT-Berufen und der Informatik-Branche stattfinden. Die Beschäftigten in der Branche müssen nicht immer männlich sein, wie uns gesellschaftliche Bilder es allzu oft vorgeben.</p><p>WER TRÄGT EIGENTLICH DIE VERANTWORTUNG FÜR ALGORITHMEN?</p><p>Wenn ein Algorithmus geschrieben wird, ist dieser erst einmal nur eine Folge von Rechenoperationen. Erst wenn Daten eingegeben werden, wenn ein Algorithmus genutzt wird, um ein bestimmtes Problem zu lösen, kommen Ergebnisse raus, die, wie in den oben beschriebenen Fällen diskriminierende Verhalten an den Tag legen können. Am Beispiel der Software „Compas“, die die US-Justiz einsetzt, um sich durch diese Software bei der Festsetzung der Katuions- und Strafhöhen oder der Entscheidung, ob eine Strafe zur Bewährung ausgesetzt wird oder nicht, werden diese Fragen deutlich. Journalisten der NGO ProPublica konnten zwar nicht rausfinden, wie der Algorithmus der Software programmiert wurde, weil dieser der Geheimhaltung unterliegt, jedoch wurden die Parameter offengelegt, die durch die Software abgefragt werden. Nach der Hautfarbe wird nicht explizit gefragt, dafür aber danach, ob die Verwandten inhaftiert sind, ob der*die Straftäter*in mit Geld umgehen kann, ob er*sie häufig umzieht, ob sich die Eltern getrennt haben. Anhand dieser Daten nimmt der Algorithmus eine Risikobewertung der angeklagten Person vor, diese wird dann dem*der Richter*in vorgelegt, der*die über das Strafmaß entscheidet. Das Verknüpfen des Risikos für Strafanfälligkeit an soziologische Faktoren wie Einkommen, Wohnort, Herkunft oder Familie ist die eine Seite. Die andere ist, dass die Software „Compas“ zwar an sich keine Variable „Hautfarbe“ erhebt, diese aber anhand von Drittvariablen, etwa dem Wohnort und dem Namen, errechnet. Der Algorithmus hat sich durch ein Deep LearningVerfahren selbst beigebracht, etwas auszurechnen, was von den Entwickler*innen und Anwender*innen mit Absicht nicht vorgesehen war. Ein solches selbst erlerntes diskriminierendes Verhalten kann nur von außen reguliert werden, die künstliche Intelligenz ihr eigenes nach unseren Maßstäben diskriminierendes Verhalten nicht wahrnehmen kann. Jedoch ist dabei zu beachten, dass dieses Verhalten bereits durch die Programmierung der Algorithmen impliziert wird und es daher bereits vorher vermieden werden muss. Denn Vorurteile, Verzerrungen und Ungerechtigkeiten erkennt erst, wer den Status Quo an einem gesellschaftlichen Ideal misst. Das müssten Menschen der künstlichen Intelligenz zuerst einmal vermitteln, in eindeutigen mathematischen Formeln. Wer trägt also in diesem Beispiel welche Verantwortung? Die Entwickler*innen der Software? Die Datenerheber*innen und Dateneingeber*innen? Die Richter*innen, die sich von dieser Risikobewertung beeinflussen lassen? Oder gar die künstliche Intelligenz selbst, die durch Lernen Entscheidungen trifft, die diskriminierend sein können?</p><p>VERANTWORTUNG VON WISSENSCHAFT UND UNTERNEHMEN</p><p>Einen Ansatzpunkt, um diese Fragen zu diskutieren, könnte eine Art „Berufsethik“ für Beschäftigte in der IT-Branche sein, wie es sie auch für Ärtz*innen oder Journalist*innen gibt. Mit einer solchen Berufsethik werden Beschäftige in der Branche auf ethisch vertretbares und nicht-diskriminierendes Programmieren hingewiesen und dazu verpflichtet, dies in ihrer Arbeit zu berücksichtigen. So wie die Wissenschaft auch in anderen Bereichen, zum Beispiel in der Genforschung, einen Verhaltenskodex über die Deutsche Forschungsgemeinschaft entwickelt hat, ist ein solcher auch für die IT-Forschung vorstellbar, zumindest dann, wenn sie nachhaltigen Einfluss auf das gesellschaftliche Zusammenleben und Individuen hat. Weiterhin kann diskutiert werden, inwieweit eine Prüfinstanz wie der TÜV in Hinblick auf Algorithmen eine Art „Zulassung“ vergeben könnte, bevor ein Algorithmus in sensiblen Bereichen wie in Bewerbungsverfahren oder in der Kreditvergabe angewendet werden darf. Deutlich klarer liegt der Fall, wenn es sich um offensichtliche Diskriminierungen handelt, denen Gesetze, wie das Antidiskriminierungsgesetz, einen Riegel vorschieben. Hier muss bei Verstößen mit dem Mittel des Strafrechts gehandelt werden, das Internet ist kein straffreier Raum. Zudem müssen Unterstützungsleistungen für Betroffene von Diskriminierung, wie Antidiskriminierungsstellen oder ehrenamtliche Vereine, ausgebaut und gefördert werden.</p><p>AUCH GOOGLE AGIERT NICHT IM “LUFTLEEREN RAUM”</p><p>Die Debatte um das Löschen von Hate-Speech in sozialen Netzwerken zeigt, wie schwierig eine Regulierung von weltweit agierenden Internetfirmen wie zum Beispiel Google, Amazon oder Facebook sein kann. Viele Frauen sind von Hate-Speech in sozialen Netzwerken betroffen. Die massiven Beleidigungen und Drohungen, denen insbesondere marginalisierte Gruppen ausgesetzt sind, die sich online politisch äußern, können dazu führen, dass Frauen aus digitalen Diskursen verdrängt werden. Dadurch wird der Zustand verstärkt, dass digitale Räume von Männern dominiert werden. Trotzdem dürfen sich diese Unternehmen genauso wenig staatlicher Kontrolle entziehen, wie jeder kleine</p><p>Juso-Bundeskongress 22. – 24. November 2019</p><p>Beschluss N2</p><p>Betrieb, wie jedes Individuum. Hier fordern wir Politik und Gesellschaft auf, sich der Herausforderung zu stellen und Regelungsmechanismen auf nationaler, europäischer und internationaler Ebene anzustreben, dass eben nicht alles unreguliert im luftleeren Raum des Digitalen geschehen kann, ohne das Verantwortung übernommen wird. Spezifisch in Deutschland geltenes Recht hinsichtlich Straftaten wie Beleidigungen oder Androhung von Straftaten, sind auch in sozialen Netzwerken konsequent zu verfolgen. Die staatliche Verantwortung zur Strafverfolgung darf im Netz nicht den Plattformbetreiber*innen übertragen werden. Dazu gehören auch Fragen des Datenschutzes, die aktuell häufig so geregelt sind, dass sie technischen Entwicklungen mehrere Jahre hinterherlaufen. Hierbei ist sicherzustellen, dass Datenschutzgesetze für Nutzer*innen leicht verständlich dargelegt werden. Ebenso fordern wir, dass es auch international verpflichtend wird, dem Prinzip ’Privacy-by-Default’ zu folgen, wie es in der EU bereits der Fall ist. Dieser Grundsatz schreibt vor, dass die voreingestellte Zustimmung zur Preis- oder Weitergabe von Daten (beispielsweise bei der Anmeldung auf Facebook) stets die ist, die den höchstmöglichen Datenschutz, also meist die kleinstmögliche Weitergabe von Daten, für die Nutzer*innen darstellt. Des Weiteren soll eine internationale Institution geschaffen werden, deren Aufgaben insbesondere die Begleitung und Beobachtung der technischen Entwicklung, die Kontrolle der Einhaltung des geltenden Rechts und ethischer und moralischer Grundsätze sowie der Ermittlung einer nachhaltigen Sanktionierung des Fehlverhaltens künstlicher Intelligenz und der Vermeidung von künftigen Fehlern sein sollen. Eine faire Zurechnung des Verhaltens der KI an ihre Entwickler*innen oder Rechteinhaber*innen soll dabei ermöglicht werden, und dabei einen Ausgleich zwischen der Durchsetzung des geltenden Rechts sowie der Fortentwicklung künstlicher Intelligenz schaffen.</p><p>WO WOLLEN WIR DIE GRENZE(N) ZIEHEN?</p><p>Algorithmen sind in unserem alltäglichen Umfeld schon längst in Bereiche vorgedrungen, die wir zum einen lange Zeit als privat markiert haben, zum anderen haben sie reelle Auswirkungen auf uns als Individuen, auf gesellschaftliches Zusammenleben und auf die Wahrnehmung und Gestaltung unserer Umwelt. Wir müssen eine gesellschaftliche Debatte anstoßen, in der wir darüber diskutieren, was Maschinen können und dürfen sollen, wo die Grenzen der Anwendung von Algorithmen liegen.</p><p>Es geht um nichts geringeres, als die Schaffung von Rahmenbedingungen für das zukünftige gesellschaftliche Zusammenleben. Für die zu führende Debatte geben wir uns folgenden grundsätzlichen Forderungen als Leitlinien:</p><p>Der Menschenwürde- und Gleichheitsgrundsatz muss auch in der digitalisierten Welt gelten. Bei der Verwendung von algorithmusgestützen, bzw. algorithmisch determinierten Entscheidungsverfahren ist sicherzustellen, dass kein Mensch vom Zugang zu Gütern, Dienstleistung oder der Teilhabe am gesellschaftlichen und kulturellen Leben ausgeschlossen wird. Insbesondere muss dies in den Bereichen Gesundheit, Justiz, Polizei, Freizügigkeit, Wohnen, Arbeit, Gleichstellung und dem Schutz vor elementaren Lebensrisiken gelten.</p><p>Ethisch-Normative Prinzipien können und dürfen nur von Menschen aufgestellt werden.</p><p>Automatisierte Entscheidungen sind von Personen zu verantworten. Die Kriterien einer automatisierten Entscheidungsfindung sind soweit wie möglich offenzulegen. Entscheidungen, die in Grundrechte Betroffener eingreifen, sind von Menschen zu treffen.</p><p>Wer von einer automatisierten Entscheidung in einem Bereich von erheblicher Bedeutung für die Lebensführungen betroffen ist, genießt einen Rechtsanspruch auf unabhängige Überprüfung und ggf. einer Korrektur der Entscheidung durch Menschen.</p><p>Benachteiligte oder schutzbedürftige Menschen, sowie Kinder und Heranwachsende genießen einen besonderen Schutz.</p><p>Juso-Bundeskongress 22. – 24. November 2019</p><p>O Öffentliche Daseinsvorsorge, Mieten & Kommunales</p><p>Juso-Bundeskongress 22. – 24. November 2019</p><p>Beschluss O1</p>"}